#!/usr/bin/env python3
"""
ç©·æ¸¸ç½‘æ•°æ®å¯¼å…¥è„šæœ¬
å°†çˆ¬å–çš„æ™¯ç‚¹æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“ä¸­
"""

import asyncio
import sys
import os
import json
import re
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.database import get_async_session_local
from app.core.logging_config import setup_logging
from app.models.attraction_detail import AttractionDetail
from loguru import logger


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤ç©ºæ ¼å’Œæ¢è¡Œ"""
    if not text:
        return ""
    # å»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
    cleaned = re.sub(r'\s+', '', text)
    return cleaned.strip()


def clean_address(address: str) -> str:
    """æ¸…ç†åœ°å€ï¼šå»é™¤(æŸ¥çœ‹åœ°å›¾)ç­‰å¤šä½™æ–‡å­—"""
    if not address:
        return ""
    # å»é™¤ (æŸ¥çœ‹åœ°å›¾) åŠå…¶å˜ä½“
    cleaned = re.sub(r'\(æŸ¥çœ‹åœ°å›¾\)', '', address, flags=re.IGNORECASE)
    cleaned = re.sub(r'\(æŸ¥çœ‹åœ°åœ–\)', '', cleaned, flags=re.IGNORECASE)
    return cleaned.strip()


def parse_data(data: List[Dict[str, Any]], destination: str = "æ­å·", city: str = "æ­å·å¸‚") -> List[Dict[str, Any]]:
    """
    è§£æç©·æ¸¸ç½‘æ•°æ®ï¼ŒæŒ‰æ™¯ç‚¹åç§°åˆ†ç»„
    
    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
        destination: ç›®çš„åœ°
        city: åŸå¸‚
        
    Returns:
        è§£æåçš„æ™¯ç‚¹æ•°æ®åˆ—è¡¨
    """
    # æŒ‰æ ‡é¢˜åˆ†ç»„æ•°æ®
    grouped_data: Dict[str, Dict[str, Any]] = {}
    
    for item in data:
        title = clean_text(item.get("æ ‡é¢˜", ""))
        if not title:
            continue
        
        # åˆå§‹åŒ–æ™¯ç‚¹æ•°æ®
        if title not in grouped_data:
            grouped_data[title] = {
                "name": title,
                "destination": destination,
                "city": city,
                "image_url": item.get("å›¾ç‰‡", ""),
                "address": None,
                "opening_hours_text": None,
                "price_note": None,
                "phone": None,
                "website": None,
                "level": item.get("çº§åˆ«", ""),  # è¯„åˆ†çº§åˆ«ï¼Œå¯ä»¥å­˜åˆ°extra_info
                "review_count": None,  # ç‚¹è¯„æ•°ï¼Œå¯ä»¥å­˜åˆ°extra_info
            }
        
        # è·å–æ•°æ®åå’Œå†…å®¹
        data_name = clean_text(item.get("æ•°æ®å", ""))
        content = item.get("å†…å®¹", "").strip()
        
        if not content:
            continue
        
        # æ ¹æ®æ•°æ®ååˆ†ç±»å¤„ç†
        if "åœ°å€" in data_name:
            grouped_data[title]["address"] = clean_address(content)
        elif "å¼€æ”¾æ—¶é—´" in data_name or "å¼€æ”¾æ™‚é–“" in data_name:
            grouped_data[title]["opening_hours_text"] = content
        elif "é—¨ç¥¨" in data_name:
            # é—¨ç¥¨å­˜åˆ°price_note
            if grouped_data[title]["price_note"]:
                grouped_data[title]["price_note"] += f"ï¼›{content}"
            else:
                grouped_data[title]["price_note"] = content
        elif "ç”µè¯" in data_name or "é›»è©±" in data_name:
            grouped_data[title]["phone"] = content
        elif "åˆ°è¾¾æ–¹å¼" in data_name or "åˆ°é”æ–¹å¼" in data_name:
            # åˆ°è¾¾æ–¹å¼å¯ä»¥å­˜åˆ°extra_info
            if "extra_info" not in grouped_data[title]:
                grouped_data[title]["extra_info"] = {}
            grouped_data[title]["extra_info"]["transportation"] = content
        
        # æå–ç‚¹è¯„æ•°
        dping_text = item.get("dping", "")
        if dping_text:
            match = re.search(r'(\d+)', dping_text)
            if match:
                grouped_data[title]["review_count"] = int(match.group(1))
    
    # è½¬æ¢ä¸ºåˆ—è¡¨
    result = []
    for title, att_data in grouped_data.items():
        # å¤„ç†extra_info
        extra_info = att_data.pop("extra_info", {})
        level = att_data.pop("level", None)
        review_count = att_data.pop("review_count", None)
        
        if level:
            extra_info["rating_level"] = level
        if review_count:
            extra_info["review_count"] = review_count
        
        if extra_info:
            att_data["extra_info"] = extra_info
        
        # ç§»é™¤Noneå€¼çš„å­—æ®µ
        cleaned_data = {k: v for k, v in att_data.items() if v is not None}
        result.append(cleaned_data)
    
    return result


async def insert_attraction_details(data_file: str, destination: str = "æ­å·", city: str = "æ­å·å¸‚", dry_run: bool = False):
    """
    å¯¼å…¥æ™¯ç‚¹è¯¦ç»†ä¿¡æ¯åˆ°æ•°æ®åº“
    
    Args:
        data_file: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
        destination: ç›®çš„åœ°
        city: åŸå¸‚
        dry_run: æ˜¯å¦åªæ˜¯é¢„è§ˆä¸å®é™…æ’å…¥
    """
    # è¯»å–JSONæ–‡ä»¶
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        logger.info(f"âœ… æˆåŠŸè¯»å–æ•°æ®æ–‡ä»¶: {data_file}ï¼Œå…± {len(raw_data)} æ¡åŸå§‹è®°å½•")
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # è§£ææ•°æ®
    parsed_data = parse_data(raw_data, destination=destination, city=city)
    logger.info(f"âœ… è§£æå®Œæˆï¼Œå…± {len(parsed_data)} ä¸ªæ™¯ç‚¹")
    
    # é¢„è§ˆæ•°æ®
    logger.info("\nğŸ“‹ é¢„è§ˆå‰5ä¸ªæ™¯ç‚¹æ•°æ®:")
    for i, item in enumerate(parsed_data[:5], 1):
        logger.info(f"\n{i}. {item.get('name', '')}")
        address = item.get('address') or ''
        opening_hours = item.get('opening_hours_text') or ''
        price_note = item.get('price_note') or ''
        image_url = item.get('image_url') or ''
        logger.info(f"   åœ°å€: {address}")
        logger.info(f"   å¼€æ”¾æ—¶é—´: {opening_hours}")
        logger.info(f"   é—¨ç¥¨: {price_note}")
        logger.info(f"   å›¾ç‰‡: {image_url}")
    
    if dry_run:
        logger.info("\nğŸ” é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…æ’å…¥æ•°æ®")
        return
    
    # æ’å…¥æ•°æ®åº“
    async_session_factory = get_async_session_local()
    created_count = 0
    updated_count = 0
    skipped_count = 0
    
    async with async_session_factory() as db:
        try:
            from sqlalchemy import select
            
            for item in parsed_data:
                name = item.get("name")
                if not name:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ ¹æ®åç§°å’Œç›®çš„åœ°ï¼‰
                result = await db.execute(
                    select(AttractionDetail).where(
                        AttractionDetail.name == name,
                        AttractionDetail.destination == destination
                    )
                )
                existing = result.scalar_one_or_none()
                
                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    for key, value in item.items():
                        if key not in ["name", "destination"] and value is not None:
                            setattr(existing, key, value)
                    updated_count += 1
                    logger.debug(f"æ›´æ–°: {name}")
                else:
                    # åˆ›å»ºæ–°è®°å½•
                    new_detail = AttractionDetail(**item)
                    db.add(new_detail)
                    created_count += 1
                    logger.debug(f"åˆ›å»º: {name}")
            
            await db.commit()
            logger.info(f"\nâœ… å¯¼å…¥å®Œæˆï¼")
            logger.info(f"   åˆ›å»º: {created_count} æ¡")
            logger.info(f"   æ›´æ–°: {updated_count} æ¡")
            logger.info(f"   è·³è¿‡: {skipped_count} æ¡")
            
        except Exception as e:
            await db.rollback()
            logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç©·æ¸¸ç½‘æ•°æ®å¯¼å…¥è„šæœ¬")
    parser.add_argument(
        "data_file",
        type=str,
        help="JSONæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¾‹å¦‚: tmp/æ­å·.jsonï¼‰"
    )
    parser.add_argument(
        "--destination",
        type=str,
        default="æ­å·",
        help="ç›®çš„åœ°ï¼ˆé»˜è®¤: æ­å·ï¼‰"
    )
    parser.add_argument(
        "--city",
        type=str,
        default="æ­å·å¸‚",
        help="åŸå¸‚ï¼ˆé»˜è®¤: æ­å·å¸‚ï¼‰"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…æ’å…¥æ•°æ®"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.data_file):
        logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_file}")
        sys.exit(1)
    
    # æ‰§è¡Œå¯¼å…¥
    await insert_attraction_details(
        data_file=args.data_file,
        destination=args.destination,
        city=args.city,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    # é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…æ’å…¥ï¼Œåªçœ‹è§£æç»“æœï¼‰
    # python scripts/qyer_data_insert.py tmp/æ­å·.json --dry-run

    # å®é™…å¯¼å…¥ï¼ˆé»˜è®¤ç›®çš„åœ°ä¸º"æ­å·"ï¼ŒåŸå¸‚ä¸º"æ­å·å¸‚"ï¼‰
    # python scripts/qyer_data_insert.py tmp/æ­å·.json

    # æŒ‡å®šç›®çš„åœ°å’ŒåŸå¸‚
    # python scripts/qyer_data_insert.py tmp/æ­å·.json --destination æ­å· --city æ­å·å¸‚
    asyncio.run(main())

